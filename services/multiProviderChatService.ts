/**
 * Multi-Provider Chat Service with Automatic Fallback
 *
 * Provider Priority Chain:
 * 1. Ollama kimi-k2:1t-cloud (Primary)
 * 2. Ollama qwen-coder:480b-cloud (Secondary)
 * 3. Google Gemini 2.0 Flash (Tertiary)
 * 4. OpenAI GPT-4o (Quaternary)
 *
 * Features:
 * - Automatic provider failover
 * - Feature-optimized routing (e.g., Gemini for web search)
 * - Conversation continuity across provider switches
 * - Detailed logging for debugging
 */

import { AIProvider } from '../types';
import * as UnifiedAPI from '../api/unified';

export interface ChatMessage {
  role: 'system' | 'user' | 'assistant';
  content: string;
}

export interface SearchResult {
  title: string;
  snippet: string;
  link: string;
}

export interface ChatResponse {
  text: string;
  sources?: SearchResult[];
  provider?: AIProvider;
  model?: string;
}

/**
 * Chat class with automatic multi-provider fallback
 */
export class Chat {
  private messages: ChatMessage[] = [];
  private systemInstruction?: string;
  private temperature: number;
  private topP: number;
  private lastSuccessfulProvider?: AIProvider;

  constructor(
    systemInstruction?: string,
    options?: {
      temperature?: number;
      topP?: number;
    }
  ) {
    this.systemInstruction = systemInstruction;
    this.temperature = options?.temperature ?? 0.7;
    this.topP = options?.topP ?? 0.9;

    console.log('ğŸ¤– Multi-Provider Chat initialized');
    console.log(`ğŸ“Š Temperature: ${this.temperature}, Top-P: ${this.topP}`);
  }

  /**
   * Send a message with automatic provider fallback
   */
  async sendMessage(userMessage: string): Promise<string> {
    console.log(`ğŸ’¬ Sending message (${this.messages.length} messages in history)`);

    this.messages.push({
      role: 'user',
      content: userMessage,
    });

    try {
      const requestMessages: UnifiedAPI.UnifiedMessage[] = [];

      if (this.systemInstruction) {
        requestMessages.push({
          role: 'system',
          content: this.systemInstruction,
        });
      }

      requestMessages.push(...this.messages);

      const response = await UnifiedAPI.chat({
        messages: requestMessages,
        temperature: this.temperature,
        topP: this.topP,
      });

      this.lastSuccessfulProvider = response.provider;

      this.messages.push({
        role: 'assistant',
        content: response.content,
      });

      console.log(`âœ… Response received from ${response.provider} (${response.model})`);

      return response.content;
    } catch (error) {
      console.error('âŒ All providers failed:', error);
      // Remove the user message since it failed
      this.messages.pop();
      throw error;
    }
  }

  /**
   * Analyze an image with automatic provider fallback
   * Priority: Gemini (best vision) â†’ GPT-4o â†’ Ollama (llava)
   */
  async analyzeImage(
    imageBase64: string,
    prompt: string
  ): Promise<string> {
    console.log('ğŸ–¼ï¸ Analyzing image with multi-provider fallback');

    try {
      const response = await UnifiedAPI.analyzeImage(imageBase64, prompt, {
        temperature: this.temperature,
        topP: this.topP,
      });

      console.log(`âœ… Image analyzed by ${response.provider} (${response.model})`);

      return response.content;
    } catch (error) {
      console.error('âŒ Image analysis failed on all providers:', error);
      throw error;
    }
  }

  /**
   * Get conversation history
   */
  getHistory(): ChatMessage[] {
    return [...this.messages];
  }

  /**
   * Clear conversation history
   */
  clearHistory(): void {
    this.messages = [];
    console.log('ğŸ—‘ï¸ Conversation history cleared');
  }

  /**
   * Get the last successful provider
   */
  getLastProvider(): AIProvider | undefined {
    return this.lastSuccessfulProvider;
  }
}

/**
 * Perform a web search with automatic fallback
 * Prioritizes Gemini (has built-in web search with sources)
 */
export async function performSearch(
  query: string,
  options?: {
    temperature?: number;
    topP?: number;
    maxLength?: number;
  }
): Promise<ChatResponse> {
  const maxLength = options?.maxLength || 500;

  console.log('ğŸ” Performing search with multi-provider fallback');
  console.log(`ğŸ“ Max length: ${maxLength} words`);

  // Try Gemini first (has web search capability)
  const GEMINI_API_KEY = process.env.GEMINI_API_KEY || process.env.API_KEY || '';

  if (GEMINI_API_KEY) {
    try {
      console.log('ğŸ”„ Trying Gemini with web search (ä¼˜å…ˆé€‰æ‹©)');
      const response = await performSearchWithGemini(query, maxLength, options);
      console.log('âœ… Success with Gemini web search');
      return response;
    } catch (error) {
      console.log('âš ï¸ Gemini search failed, falling back to Ollama');
    }
  }

  // Fallback to Ollama (knowledge-based, no web search)
  try {
    console.log('ğŸ”„ Trying Ollama (knowledge-based)');
    const response = await performSearchWithOllama(query, maxLength, options);
    console.log('âœ… Success with Ollama');
    return response;
  } catch (error) {
    console.log('âš ï¸ Ollama failed, trying GPT-4o');
  }

  // Final fallback to GPT-4o
  try {
    console.log('ğŸ”„ Trying GPT-4o');
    const response = await performSearchWithOpenAI(query, maxLength, options);
    console.log('âœ… Success with GPT-4o');
    return response;
  } catch (error) {
    throw new Error('Search failed on all providers');
  }
}

/**
 * Search with Gemini (has web search)
 */
async function performSearchWithGemini(
  query: string,
  maxLength: number,
  options?: { temperature?: number; topP?: number }
): Promise<ChatResponse> {
  const { GoogleGenerativeAI } = await import('@google/generative-ai');
  const GEMINI_API_KEY = process.env.GEMINI_API_KEY || process.env.API_KEY || '';

  const genAI = new GoogleGenerativeAI(GEMINI_API_KEY);
  const model = genAI.getGenerativeModel({
    model: 'gemini-2.0-flash-exp',
    generationConfig: {
      temperature: options?.temperature ?? 0.7,
      topP: options?.topP ?? 0.9,
    },
  });

  const chat = model.startChat({
    history: [],
    tools: [{
      googleSearch: {}
    }],
  });

  const enhancedQuery = `è«‹å›ç­”ä»¥ä¸‹å•é¡Œï¼ˆé™åˆ¶åœ¨ ${maxLength} å­—ä»¥å…§ï¼‰ï¼š

${query}

è«‹æä¾›æº–ç¢ºã€ç°¡æ½”çš„å›ç­”ï¼Œä¸¦åœ¨é©ç•¶æ™‚å¼•ç”¨ä¾†æºã€‚`;

  const result = await chat.sendMessage(enhancedQuery);
  const response = result.response;

  // Extract grounding sources if available
  const sources: SearchResult[] = [];
  const groundingMetadata = (response as any).candidates?.[0]?.groundingMetadata;

  if (groundingMetadata?.groundingSupports) {
    for (const support of groundingMetadata.groundingSupports) {
      if (support.groundingChunkIndices && support.segment) {
        for (const chunkIndex of support.groundingChunkIndices) {
          const chunk = groundingMetadata.groundingChunks?.[chunkIndex];
          if (chunk?.web) {
            sources.push({
              title: chunk.web.title || chunk.web.uri,
              snippet: support.segment.text || '',
              link: chunk.web.uri,
            });
          }
        }
      }
    }
  }

  return {
    text: response.text(),
    sources: sources.length > 0 ? sources : undefined,
    provider: AIProvider.GEMINI,
    model: 'gemini-2.0-flash-exp',
  };
}

/**
 * Search with Ollama (knowledge-based, no web search)
 */
async function performSearchWithOllama(
  query: string,
  maxLength: number,
  options?: { temperature?: number; topP?: number }
): Promise<ChatResponse> {
  const enhancedQuery = `è«‹å›ç­”ä»¥ä¸‹å•é¡Œï¼ˆé™åˆ¶åœ¨ ${maxLength} å­—ä»¥å…§ï¼‰ï¼š

${query}

æ³¨æ„ï¼šè«‹åŸºæ–¼ä½ çš„çŸ¥è­˜æä¾›æº–ç¢ºçš„å›ç­”ã€‚å¦‚æœä¸ç¢ºå®šï¼Œè«‹æ˜ç¢ºèªªæ˜ã€‚`;

  const response = await UnifiedAPI.chat({
    messages: [
      { role: 'user', content: enhancedQuery }
    ],
    temperature: options?.temperature ?? 0.7,
    topP: options?.topP ?? 0.9,
    model: 'kimi-k2:1t-cloud', // Use primary model
  });

  return {
    text: response.content,
    provider: response.provider,
    model: response.model,
  };
}

/**
 * Search with OpenAI GPT-4o
 */
async function performSearchWithOpenAI(
  query: string,
  maxLength: number,
  options?: { temperature?: number; topP?: number }
): Promise<ChatResponse> {
  const enhancedQuery = `è«‹å›ç­”ä»¥ä¸‹å•é¡Œï¼ˆé™åˆ¶åœ¨ ${maxLength} å­—ä»¥å…§ï¼‰ï¼š

${query}

è«‹æä¾›æº–ç¢ºã€ç°¡æ½”çš„å›ç­”ã€‚`;

  const response = await UnifiedAPI.chat({
    messages: [
      { role: 'user', content: enhancedQuery }
    ],
    temperature: options?.temperature ?? 0.7,
    topP: options?.topP ?? 0.9,
  });

  return {
    text: response.content,
    provider: response.provider,
    model: response.model,
  };
}

/**
 * Analyze text with automatic fallback
 */
export async function analyzeText(
  prompt: string,
  options?: {
    temperature?: number;
    topP?: number;
    systemInstruction?: string;
  }
): Promise<ChatResponse> {
  console.log('ğŸ“ Analyzing text with multi-provider fallback');

  try {
    const messages: UnifiedAPI.UnifiedMessage[] = [];

    if (options?.systemInstruction) {
      messages.push({
        role: 'system',
        content: options.systemInstruction,
      });
    }

    messages.push({
      role: 'user',
      content: prompt,
    });

    const response = await UnifiedAPI.chat({
      messages,
      temperature: options?.temperature ?? 0.7,
      topP: options?.topP ?? 0.9,
    });

    console.log(`âœ… Text analyzed by ${response.provider} (${response.model})`);

    return {
      text: response.content,
      provider: response.provider,
      model: response.model,
    };
  } catch (error) {
    console.error('âŒ Text analysis failed on all providers:', error);
    throw error;
  }
}

/**
 * Analyze an image (standalone function)
 */
export async function analyzeImage(
  imageBase64: string,
  prompt: string,
  options?: {
    temperature?: number;
    topP?: number;
  }
): Promise<ChatResponse> {
  console.log('ğŸ–¼ï¸ Analyzing image (standalone)');

  try {
    const response = await UnifiedAPI.analyzeImage(imageBase64, prompt, {
      temperature: options?.temperature ?? 0.7,
      topP: options?.topP ?? 0.9,
    });

    return {
      text: response.content,
      provider: response.provider,
      model: response.model,
    };
  } catch (error) {
    console.error('âŒ Image analysis failed:', error);
    throw error;
  }
}

/**
 * Check which providers are currently available
 */
export async function checkAvailableProviders(): Promise<{
  ollama: boolean;
  gemini: boolean;
  openai: boolean;
}> {
  const available = await UnifiedAPI.checkAvailableProviders();
  console.log('ğŸ¥ Provider availability:', available);
  return available;
}

/**
 * Get provider status with detailed information
 */
export async function getProviderStatus(): Promise<{
  primary: { name: string; available: boolean; model: string };
  secondary: { name: string; available: boolean; model: string };
  tertiary: { name: string; available: boolean; model: string };
  quaternary: { name: string; available: boolean; model: string };
}> {
  const available = await checkAvailableProviders();

  return {
    primary: {
      name: 'Ollama (Kimi K2)',
      available: available.ollama,
      model: 'kimi-k2:1t-cloud',
    },
    secondary: {
      name: 'Ollama (Qwen Coder)',
      available: available.ollama,
      model: 'qwen-coder:480b-cloud',
    },
    tertiary: {
      name: 'Google Gemini',
      available: available.gemini,
      model: 'gemini-2.0-flash-exp',
    },
    quaternary: {
      name: 'OpenAI GPT-4o',
      available: available.openai,
      model: 'gpt-4o',
    },
  };
}

// Export default for backward compatibility
export default {
  Chat,
  performSearch,
  analyzeText,
  analyzeImage,
  checkAvailableProviders,
  getProviderStatus,
};
